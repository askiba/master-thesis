\chapter{Wprowadzenie teoretyczne}
\label{cha:wstepTeoretyczny}

W rozdziale tym przedstawiono informacje .

%---------------------------------------------------------------------------

\section{Fizyczny model narciarza}
\label{sec:fizycznyModel}

%---------------------------------------------------------------------------

\section{Metody numeryczne rozwi±zywania równañ ró¿niczkowych}
\label{sec:numeryka}


%---------------------------------------------------------------------------

\section{Optymalizacja}
\label{sec:optymalizacja}

W tym podrozdziale opisane s± metody optymalizacji u¿yte w zaproponowanym rozwi±zaniu, czyli algorytm ewolucyjny oraz algorytm optymalizacji lokalnej - Hill climbing.

% czym jest optymalizacja
Zadaniem optymalizacji jest przeszukanie przestrzeni rozwi±zañ w celu znalezienia takiego, które jest najlepsze. Zatem maj±c dan± funkcjê, nazywan± funkcj± celu, która ka¿demu punktowi reprezentuj±cemu rozwi±zanie problemu, poszukujemy takiego, dla którego warto¶æ tej funkcji bêdzie jak najmniejsza (b±d¼ jak najwiêksza). Trudno¶æ w znalezieniu takiego rozwi±zania zale¿y od charakteru funkcji celu, a czasem tak¿e od nieznajomo¶ci jej analitycznej postaci.

\subsubsection{Optymalizacja lokalna i globalna}
W przypadku funkcji z jednym optimum do znalezienia najlepszego rozwi±zania wystarczy przeszukiwanie lokalne. Polega ono na iteracyjnym sprawdzaniu rozwi±zañ w najbli¿szej przestrzeni i wprowadzaniu lokalnych zmian, aby w koñcu znale¼æ rozwi±zanie najlepsze w okolicy tzw. optimum lokalne. Je¶li wiemy, ¿e istnieje tylko jedno takie optimum, mo¿emy mieæ pewno¶æ, ¿e znalezione rozwi±zanie jest najlepszym w ca³ej przestrzeni rozwi±zañ. Przyk³adami optymalizacji lokalnych s±:
\begin{itemize}
\item hill climbing
\item przeszukiwanie tabu
\end{itemize}

Je¶li natomiast funkcja celu posiada wiele optimów lokalnych (tzw. funkcja wielomodalna) to optymalizacjê nazywamy optymalizacj± globaln±. Je¶li zadanie jest ci±g³e, a wiêc niemo¿liwe jest przeszukanie ca³ej przestrzeni rozwi±zañ, nigdy nie mo¿emy byæ pewni, ¿e zastosowany algorytm optymalizacji da nam rozwi±zanie najlepsze - byæ mo¿e bêdzie to tylko minimum lokalne a nie globalne. Nie maj±c takiej pewno¶ci nie wiemy kiedy nale¿y zatrzymaæ algorytm. Z tego powodu stosuje siê parametr steruj±cy czasem trwania obliczeñ, kosztem mniejszej pewno¶ci co do poprawno¶ci rozwi±zania mo¿emy otrzymaæ krótszy czas optymalizacji i odwrotnie.

\subsection{Algorytm ewolucyjny}
\label{sec:ewolucyjny}
Algorytm ewolucyjny jest przyk³adem algorytmu optymalizacyjnego, przeszukuj±cego przestrzeñ rozwi±zañ w celu znalezienia najlepszego rozwi±zania problemu. Algorytm ten oparty jest na obserwacjach ¶rodowiska i przystosowywania siê organizmów do jego warunków. Wiele terminów zapo¿yczonych jest zatem z genetyki.

Podstaw± ca³ego algorytmu jest populacja osobników, z których ka¿dy reprezentuje rozwi±zanie problemu. Populacja ta zmienia siê wraz z dzia³aniem algorytmu. Ewolucja zak³ada, ¿e populacja bêdzie siê sk³adaæ z coraz lepiej przystosowanych osobników. Przystosowanie to jest obliczane za pomoc± wcze¶niej okre¶lonej funkcji oceniaj±cej jako¶æ danego osobnika, czyli jak dobre jest rozwi±zanie reprezentowane przez niego. Przystosowanie jest warto¶ci± liczbow± obliczon± za pomoc± tej funkcji przystosowania.\\
Funkcja przystosowania okre¶la warto¶æ przystosowania osobnika na podstawie jego fenotypu, który jest tworzony z genotypu. Genotyp okre¶la zestaw cech danego osobnika i sk³ada siê z chromosomów (najczê¶ciej z jednego). Natomiast ka¿dy z chromosomów sk³ada siê z elementarnych jednostek - genów.\\

\subsubsection{Schemat dzia³a algorytmu ewolucyjnego}
Algorytm ewolucyjny rozpoczyna siê poprzez wygenerowanie populacji bazowej oraz obliczenie przystosowania jej osobników. Przewa¿nie osobniki te generowane s± ca³kowicie losowo, ale mo¿na tak¿e wprowadziæ konkretne osobniki np. o znanym dobrym przystosowaniu do ¶rodowiska.

G³ówna czê¶æ algorytmu opiera siê na powtarzaniu pêtli, w której wykonywane s± kolejno:

\begin{itemize}
\item reprodukcja
\item operacje genetyczne
\item ocena
\item sukcesja
\end{itemize}

Czêsto reprodukcjê i sukcesjê ³±czy siê pod nazw± selekcja.

Reprodukcja powoduje powielenie losowo wybranych osobników z populacji. Prawdopodobieñstwo wybrania osobnika do powielenia najczê¶ciej jest proporcjonalne do jego przystosowania. Mo¿e siê zdarzyæ, ¿e dany osobnik zostanie wybrany wiêcej ni¿ raz, a tak¿e, ¿e nie zostanie wybrany ani razu.\\
Nastêpnie na tych kopiach przeprowadzane s± operacje genetyczne powoduj±ce zmiany w genotypie osobników. Wyró¿niamy dwie podstawowe operacje:

\begin{itemize}
\item mutacja
\item krzy¿owanie
\end{itemize}

%---- WSTAWKA ANGIELSKA!!!!!!!!!!!!!!!!!--------
Zadaniem mutacji jest losowe zmodyfikowanie genów w genotypie.\\
Krzy¿owanie, zwane tak¿e rekombinacj± (ang. \textit{crossover}), dzia³a na co najmniej dwóch osobnikach i na podstawie ich genotypu tworzy jeden lub wiêcej osobników potomnych. Chromosomy rodzicielskie s± mieszane w celu otrzymania nowych genotypów dla osobników potomnych.

W wyniku operacji genetycznych powstaj± nowe osobniki, które wchodz± w sk³ad populacji potomnej. Ka¿dy z tych osobników jest oceniany za pomoc± funkcji przystosowania. Porównuj±c jako¶æ osobników z populacji bazowej oraz potomnej dokonuje siê sukcesji, czyli wyboru osobników z tych populacji (czasem wy³±cznie z populacji potomnej) i tworzy now± populacjê bazow±.

Zakoñczenie dzia³ania algorytmu przewa¿nie opiera siê na badaniu funkcji przystosowania ca³ej populacji. Je¶li warto¶æ przystosowania populacji nie jest zró¿nicowana mówimy o stagnacji algorytmu i mo¿e byæ to wskazaniem do zakoñczenia dzia³ania algorytmu. Czasem jednak oczekuje siê a¿ przystosowanie to bêdzie wystarczaj±co du¿e, ¿eby stwierdziæ, ¿e znalezione rozwi±zanie jest bardzo dobre. Przewa¿nie jednak nie znamy nawet przybli¿onej warto¶ci jako¶ci rozwi±zania, wiêc nie mo¿emy stwierdziæ kiedy przystosowanie jest odpowiednie i czy nie mo¿e siê jeszcze znacznie poprawiæ.

\subsubsection{Kodowanie osobników}
W przypadku algorytmów genetycznych, bêd±cych szczególnym przypadkiem algorytmów ewolucyjnych, do kodowania osobników stosuje siê kodowanie binarne chromosomów. Pojedynczy bit reprezentuje zatem gen nale¿±cy do chromosomu.\\
W takim przypadku mutacja wykonywana jest na ka¿dym genie osobno z pewnym prawdopodobieñstwem, je¶li do niej dochodzi, zmienia siê warto¶æ bitu na przeciwn±. W krzy¿owaniu wybiera siê dwa osobniki rodzicielskie, których chromosomy rozcinane s± na dwie czê¶ci i ³±czone "na krzy¿". Miejsce przeciêcia jest losowane z rozk³adem równomiernym.

W algorytmach ewolucyjnych porzuca siê kodowanie binarne - chromosom sk³ada siê z jednej lub wiêcej liczb stanowi±cych cechy osobnika.\\
Mutacja takiego osobnika najczê¶ciej odbywa siê poprzez losow± zmianê ka¿dej z warto¶ci genów chromosomu. Do krzy¿owania wybiera siê dwa osobniki, z których dla ka¿dej pary odpowiadaj±cych genów wyci±gana jest ¶rednia i tak otrzymane warto¶ci genów tworz± genotyp nowego osobnika.

\subsubsection{Typy algorytmów ewolucyjnych}
Algorytmy ewolucyjne wywodz± siê z kilku osobnych nurtów zajmuj±cych siê t± tematyk±, wiêc istnieje wiele podobnych schematów. Najlepiej traktowaæ algorytmy ewolucyjne jako metaheurystykê - okre¶lony jest pewien szkic algorytmu, który mo¿na dostosowywaæ do konkretnego rozwi±zania. W tym podrozdziale opisane s± podstawowe i najbardziej popularne schematy postêpowania oparte o algorytmy ewolucyjne.

\paragraph{Prosty algorytm genetyczny}

Prosty algorytm genetyczny zosta³ zaproponowany w roku 1975 przez John'a Holland'a.

Maj±c populacjê bazow± $P^t$ dokonujemy reprodukcji tej populacji, tworz±c populacjê tymczasow± $T^t$ sk³adaj±c± siê z takiej samej liczby osobników. Wybierani s± oni z prawdopodobieñstwem proporcjonalnym do ich przystosowania z populacji bazowej. Na populacji tymczasowej dokonujemy operacji genetycznych (mutacji i krzy¿owania). Do krzy¿owania wybierane s± roz³±czne pary osobników i z pewnym prawdopodobieñstwem $p_c$ zachodzi ich skrzy¿owanie. Je¶li dosz³o do powstania osobników potomnych zastêpuj± one osobniki rodzicielskie. Nastêpnie na tak otrzymanej populacji tymczasowej dochodzi do mutacji osobników i otrzymania populacji potomnej $O^t$. Ta populacja staje siê w nastêpnej iteracji algorytmu now± populacj± bazow±.\\
Zatrzymanie algorytmu mo¿e byæ dokonane je¶li np.:

\begin{itemize}
\item wykonano okre¶lon± z góry liczbê iteracji
\item znaleziono osobnika o wystarczaj±co wysokiej warto¶ci przystosowania
\end{itemize}

W tej wersji algorytmu czêsto pêtlê algorytmu nazywa siê generacj±, a ka¿d± populacjê $P^t$ w chwili t pokoleniem.\\

\paragraph{Strategia (1+1)}

Strategia (1+1) jest podstawow± strategii ewolucyjnych. W algorytmie tym mamy do czynienia z populacj± sk³adaj±c± siê z tylko jednego osobnika posiadaj±cego jeden chromosom. W ka¿dej pêtli algorytmu dokonuje siê mutacji tego chromosomu, co powoduje powstanie nowego osobnika. Osobnik ten jest poddawany ocenie, a nastêpnie dokonuje siê wyboru lepszego z dwóch istniej±cych osobników i tego pozostawia w populacji.\\
W mutacji dodaje siê do ka¿dego genu chromosomu losow± modyfikacjê rozk³adem normalnym:
\begin{equation}
Y^t_i = X^t_i + \sigma\xi_{N(0,1),i}
\end{equation}

Warto¶æ $\sigma$ bêdzie powodowa³a wiêksze lub mniejsze zmiany w chromosomie. Je¶li chcemy przeszukaæ przestrzeñ, powinni¶my zwiêkszaæ jej warto¶æ, co jest po¿±dane zw³aszcza w pocz±tkowej fazie dzia³ania algorytmu. Natomiast, aby znale¼æ jak najlepsze rozwi±zanie, wiedz±c ¿e obecne rozwi±zanie jest ju¿ bardzo bliskie najlepszemu, mo¿emy zmniejszaæ warto¶æ $\sigma$ przeszukuj±c tylko najbli¿sz± przestrzeñ.\\
Do wyznaczania $\sigma$ powsta³ nastêpuj±cy algorytm zwany regu³± 1/5 sukcesów:
\begin{enumerate}
\item Je¶li przez kolejnych k pêtli algorytmu mutacja powoduje powstanie lepszego osobnika w wiêcej ni¿ 1/5 wszystkich mutacji, to zwiêkszamy $\sigma$: $\sigma' = c_i \sigma$. Warto¶æ $c_i$ wyznaczona empirycznie wynosi $ \frac{1}{0.82} $
\item Gdy dok³adnie 1/5 koñczy siê sukcesem, warto¶æ $\sigma$ pozostaje bez zmian.
\item Je¶li nie zachodzi ¿adne z powy¿szych warto¶æ $\sigma$ jest zmniejszana: $\sigma' = c_d \sigma$. Gdzie $ c_d $ powinna wynosiæ $ 0.82 $
\end{enumerate}

\paragraph{Strategia ($\mu$ + $\lambda$)}

Strategia ($\mu$ + $\lambda$) jest rozwiniêciem strategii (1+1). $\mu$ oznacza ilo¶æ osobników w populacji pocz±tkowej, a $\lambda$ ile osobników jest reprodukowanych i poddawanych operacjom genetycznym. Dodatkowo, zamiast regu³y 1/5 sukcesów wprowadzono mechanizm samoczynnej adaptacji zasiêgu mutacji, a tak¿e wprowadzono operator krzy¿owania.

Oznaczenie $\mu$ + $\lambda$ oznacza, ¿e po wygenerowaniu populacji potomnej wybierane jest $\mu$ najlepszych osobników do nowej populacji bazowej - zarówno spo¶ród populacji potomnej, jak i starej populacji bazowej zawieraj±cej ³±cznie $\mu$ + $\lambda$ osobników.

W strategii tej wa¿ne jest te¿ kodowanie, do którego dodatkowo do³o¿ono równie¿ chromosom przechowuj±cy wektor $\sigma$ zawieraj±cy warto¶ci odchyleñ standardowych, które wykorzystuje siê w trakcie mutacji.\\
Po wylosowaniu warto¶ci zmiennej losowej o rozk³adzie normalnym ($\xi_{N(0,1)}$) dla ka¿dego elementu wektora $\sigma$ losujemy jeszcze jedn± zmienn± losow± o rozk³adzie normalnym ($\xi_{N(0,1),i}$) i oblicza nowe warto¶ci odchyleñ z wektora $\sigma$:

\begin{equation}
\sigma'_i = \sigma_i e^{(\tau'\xi_{N(0,1)} + \tau\xi_{N(0,1),i})}
\end{equation}

%--------CO Z K i n??-------
Gdzie $\tau$ oraz $\tau'$ s± parametrami algorytmu, a ich warto¶ci powinny wynosiæ:
\begin{equation}
\tau = \frac{K}{\sqrt{2n}}
\end{equation}

\begin{equation}
\tau' = \frac{K}{\sqrt{2\sqrt{n}}}
\end{equation}

Maj±c dane nowe warto¶ci odchyleñ standardowych mo¿emy obliczyæ nowe warto¶ci genów korzystaj±c ze wzoru:

\begin{equation}
X'_i = X_i + \sigma'_i\xi_{N(0,1),i}
\end{equation}
gdzie $\xi_{N(0,1),i}$ jest now± losow± warto¶ci±.

Algorytm ewolucyjny wybiera osobniki lepiej przystosowane, a wiêc te, które posiadaj± tak¿e lepsze warto¶ci odchyleñ standardowych. Powoduje to naturaln± selekcjê, doprowadzaj±c± do samoczynnej adaptacji odchyleñ standardowych stosowanych w trakcie mutacji.

Krzy¿owanie wystêpuje w tym algorytmie pod nazw± rekombinacja. Najczê¶ciej sprowadza siê do u¶rednienia lub wymianie warto¶ci wektorów, tak¿e wektora $\sigma$.

\paragraph{Strategia ($\mu$, $\lambda$)}
Strategia ($\mu$ + $\lambda$) posiada pewne wady, które postanowiono spróbowaæ wyeliminowaæ za pomoc± nowej strategii ($\mu$, $\lambda$). Poprzedni algorytm sprawia problemy je¶li w populacji pojawia siê osobnik o wysokiej warto¶ci przystosowania, ale posiadaj±cy zbyt du¿e (albo zbyt ma³e) warto¶ci odchyleñ standardowych. Usuniêcie takiego osobnika z populacji czêsto nie jest procesem krótkotrwa³ym, gdy¿ wp³ywa on na powstaj±ce potomstwo, przekazuj±c mu podobne do jego, nieodpowiednie warto¶ci odchyleñ.\\
W nowej strategii wprowadzono zmianê, która powoduje, ¿e osobniki rodzicielskie nie s± nigdy brane do kolejnej populacji bazowej. Podczas selekcji korzysta siê zatem tylko z powsta³ej populacji potomnej, z niej wybieraj±c osobniki do populacji bazowej w kolejnej iteracji.

\subsection{Hill climbing}
\label{sec:hill}
Algorytm hill climbing jest jedn± z metod przeszukiwania lokalnego. W ka¿dej iteracji zmieniaj±c warto¶æ jednej ze zmiennych rozwi±zania sprawdzana jest warto¶æ funkcji celu dla nowego rozwi±zania i je¶li warto¶æ ta jest lepsza od dotychczas najlepszej znalezionej, zapamiêtujemy zmienione rozwi±zanie. Dopóki zmiany powoduj± poprawê rozwi±zania, algorytm nie jest zatrzymywany. Na koñcu wiemy, ¿e znalezione rozwi±zanie jest rozwi±zaniem lokalnie optymalnym.\\
Przeszukiwanie przestrzeni dyskretnej sprowadza siê do sprawdzenia rozwi±zañ najbli¿szych obecnemu i wybieranie tego rozwi±zania, którego warto¶æ obliczona za pomoc± funkcji celu jest najlepsza. Je¶li w¶ród s±siadów nie ma ju¿ lepszego rozwi±zania, mo¿emy zakoñczyæ przeszukiwanie.\\
W przestrzeni ci±g³ej konieczne jest dobranie kroku, który wyznacza punkty przeszukiwane w okolicy w trakcie ka¿dej iteracji. Dodatkowo wykorzystywane jest tzw. przyspieszenie (ang. \textit{acceleration}), które wyznacza piêciu mo¿liwych kandydatów na lepsze rozwi±zania. Najczê¶ciej przyspieszenie to wynosi 1.2, a warto¶æ kroku jest osobna dla ka¿dej zmiennej rozwi±zania i czêsto wynosi na pocz±tku 1. Zatem za ka¿dym razem obliczane s± nastêpuj±ce wspó³czynniki: -acceleration, -1/acceleration, 0, 1/acceleration, acceleration. Nastêpnie wspó³czynniki mno¿one s± przez krok (step) i dodawane do obecnie analizowanej zmiennej i wybierane jest najlepsze z piêciu rozwi±zañ. Warto¶æ kroku jest indywidualna dla ka¿dej zmiennej. Po wybraniu najlepszego rozwi±zania uaktualniana jest warto¶æ tego kroku - krok mno¿ony jest przez odpowiedni wspó³czynnik, ten który by³ dobrany wcze¶niej do znalezienia tego najlepszego rozwi±zania. Algorytm zatrzymywany jest je¶li zmiana ¿adnej ze zmiennych nie przynosi ju¿ poprawy rozwi±zania, czasem równie¿ je¶li ta zmiana jest ju¿ bardzo ma³a - wprowadzany jest parametr $\epsilon$ wyznaczaj±cy tê ró¿nicê.

%---------------------------------------------------------------------------

\section{Uczenie maszynowe}
\label{sec:maszynowe}

Uczeniem siê systemu jest ka¿da autonomiczna zmiana w systemie zachodz±ca na podstawie do¶wiadczeñ, która prowadzi do poprawy jako¶ci jego dzia³ania. (Cichosz)

Program siê uczy z do¶wiadczenia E dla zadañ T i miary jako¶ci P je¶li jego efektywno¶æ w zadaniach z T mierzona P wzrasta z do¶wiadczeniem E. (Mitchell)

Istnieje wiele rodzajów uczenia maszynowego. Podstawowy podzia³ wynika z rodzaju informacji trenuj±cej na:

\begin{itemize}
\item uczenie z nadzorem
\item uczenie bez nadzoru
\end{itemize}

W uczeniu siê z nadzorem ¼ród³em informacji trenuj±cej jest nauczyciel. Od niego otrzymuje uczeñ informacjê jakie zachowanie jest po¿±dane. Natomiast w przypadku uczenia bez nadzoru uczeñ dowiaduje siê o skuteczno¶ci swojego dzia³ania obserwuj±c wyniki - nazywa siê to czasem wbudowanym nauczycielem.

Istniej± jeszcze dwie grupy, które trudno zakwalifikowaæ do powy¿szych:

\begin{itemize}
\item uczenie siê na podstawie zapytañ
\item uczenie siê przez eksperymentowanie
\item uczenie siê ze wzmocnieniem
\end{itemize}

Do pierwszej z nich nale¿± algorytmy polegaj±ce na zadawaniu pytañ przez ucznia nauczycielowi. Natomiast do drugiej te, w których uczeñ gromadzi swoje do¶wiadczenia obserwuj±c konsekwencje swojego dzia³ania w ¶rodowisku. Uczenie siê ze wzmocnieniem jest podobne do tej metody, ale dodatkowo istnieje krytyk, który s³u¿y jako dodatkowe ¼ród³o informacji trenuj±cej. Jego zadaniem jest karanie b±d¼ nagradzanie ucznia za jego zachowanie. Uczeñ nie dowiaduje siê co ma robiæ, ale jak warto¶ciowe jest dane dzia³anie.

Czasem granice pomiêdzy tymi grupami s± nieostre i przynale¿no¶æ algorytmu do jakiej¶ grupy mo¿e zale¿eæ wy³±cznie od punktu widzenia.

\subsection{Uczenie siê ze wzmocnieniem}
W przypadku uczenia siê ze wzmocnieniem zadaniem ucznia jest obserwacja stanów ¶rodowiska, wykonywanie akcji oraz obserwowanie efektów tych akcji poprzez warto¶æ otrzymywanego wzmocnienia jako rzeczywistoliczbowej nagrody. Tak jak zosta³o to napisane wcze¶niej, w tym przypadku nie mówimy o nauczycielu, ale o krytyku, który warto¶ciuje zachowanie poprzez dostarczanie wzmocnienia. Zadaniem ucznie jest odnalezienie takiego zachowania, które przyniesie mu jak najwiêksz± nagrodê. Najczê¶ciej uczeñ nie ma pojêcia o tym jakie jest ¶rodowisko, czêsto niedeterministyczne, dlatego musi wchodziæ w interakcjê z nim, aby je poznaæ.

\begin{figure}[h]
\centering
\includegraphics{uczenie}
\end{figure}

W ka¿dym kroku uczeñ jest w okre¶lonym stanie ¶rodowiska. Decyduj±c siê na okre¶lon± akcjê otrzymuje informacjê o nowym stanie, w którym znajduje siê po wykonaniu tej akcji oraz o nagrodzie (wzmocnieniu) jak± otrzymuje za swoje dzia³anie. Uczeñ obserwuj±c nagrody otrzymywane za swoje zachowanie mo¿e uczyæ siê jak postêpowaæ, aby by³y one jak najwy¿sze.

Schemat algorytmu przedstawia siê nastêpuj±co:\\
Dla kolejnych kroków czasowych t: 
\begin{enumerate}
\item obserwujemy stan $x_t$
\item wybieramy akcjê $a_t$ mo¿liw± do wykonania w stanie $x_t$
\item wykonujemy akcjê $a_t$
\item obserwujemy wzmocnienie $r_t$ i nastêpny stan $x_{t+1}$
\item uczymy siê na podstawie do¶wiadczenia ($x_t,a_t,r_t,x_{t+1}$)
\end{enumerate}
Wybór akcji w kroku 2. dokonywany jest autonomicznie przez ucznia. Natomiast stan, do którego przechodzi po wykonaniu akcji jest okre¶lony przez ¶rodowisko na podstawie stanu poprzedniego oraz wykonanej akcji. Warto jednak zwróciæ uwagê na fakt, ¿e ¶rodowisko mo¿e byæ stochastyczne - wykonanie dwa razy tej samej akcji mo¿e dawaæ ró¿ne rezultaty. Poza tym, przewa¿nie ¶rodowisko jest nieznane uczniowi, st±d konieczno¶æ podejmowania prób i b³êdów poprzez wykonywanie ró¿nych akcji. Jednocze¶nie, uczeñ nie mo¿e wp³ywaæ na ¶rodowisko w ¿aden sposób.

\subsubsection{Strategia maksymalizacji nagród}
Nauka ucznia oparta jest na nagrodach, które otrzymuje za swoje dzia³ania. Musi znale¼æ on najlepsz± strategiê wyboru akcji, aby uzyskiwaæ jak najlepsze nagrody. Najczê¶ciej uczeñ próbuje maksymalizowaæ swoje nagrody d³ugoterminowo. Strategia ta polega na tym, ¿e nagrody za poprawne dzia³anie mog± przyj¶æ wiele kroków pó¼niej ni¿ wtedy gdy ono zosta³o wykonane. Strategia ta nazywana jest uczeniem siê z opó¼nionym wzmocnieniem. W uczeniu z natychmiastowym wzmocnieniem interesuje nas tylko maksymalizacja nagród tu¿ po danym zachowaniu. Nie jeste¶my wtedy w stanie braæ pod uwagê tego, jakie w przysz³o¶ci mog± byæ jego skutki.\\
W przypadku opó¼nionego wzmocnienia wprowadza siê wspó³czynnik dyskontowania $\gamma \in [0,1]$. Zadaniem ucznia jest zmaksymalizowanie zdyskontowanej sumy nagród:

\begin{equation}
\label{dyskont}
E[\sum_{t=0}^\infty \gamma^tr_t]
\end{equation}

Im wspó³czynnik $\gamma$ jest bli¿szy 0, tym bardziej maksymalizuje siê tylko natychmiastowe nagrody. Je¶li $\gamma = 1$ to maksymalizowana jest suma wszystkich otrzymanych nagród.

\subsubsection{Zadania epizodyczne}
W niektórych przypadkach dzia³ania ucznia mo¿na ³atwo wydzieliæ na niezale¿ne epizody, z których ka¿dy trwa najczê¶ciej skoñczon± liczbê kroków. Rozdzia³ ten jest kierowany tym, ¿e ka¿da z tych prób jest oceniania osobno. Zatem maksymalizujemy kryterium jako¶ci w ka¿dej próbie oddzielni. Zatem w równaniu \ref{dyskont} musimy zast±piæ sumê nieskoñczon± otrzymuj±c:

\begin{equation}
E[\sum_{t=0}^{n-1} \gamma^tr_t]
\end{equation}

Warto¶æ $n$ to liczba kroków epizodu. Powy¿sze równanie mo¿na traktowaæ tak naprawdê jako szczególny przypadek równania \ref{dyskont}. Zak³adaj±c, ¿e w ostatnim kroku wchodzimy do stanu, w którym jedyna mo¿liwa akcja prowadzi z powrotem do niego, a nagroda w tym stanie wynosi 0, otrzymujemy dok³adnie powy¿sze równanie. W równaniu tym zmienna $r_t$ powinna byæ dla u¶ci¶lenia zast±piona przez $r_{i,t}$, poniewa¿ warto¶ci wzmocnienia mog± byæ ró¿ne w ka¿dej próbie $i$.

Istniej± tak¿e dwa szczególnego rodzaju typy zadañ epizodycznych, które nazywane s± zadaniami do-sukcesu lub do-pora¿ki. Zakoñczenie ka¿dej próby koñczy siê odpowiednio w ka¿dym typie sukcesem lub pora¿k±. W przypadku zadañ do-sukcesu chcemy, aby w ka¿dym epizodzie w jak najmniejszej liczbie kroków osi±gn±æ pewien po¿±dany stan, co powoduje osi±gniêcie sukcesu i zakoñczenie tej próby. Odpowiednio dla zadañ do-pora¿ki jak najbardziej chcemy odwlec moment przej¶cie do niepo¿±danego stanu, który oznacza pora¿kê.

\subsubsection{Algorytm uczenia siê strategii}
\paragraph{Q-learning.}
Algorytm Q-learning jest najczê¶ciej stosowanym algorytmem do uczenia siê optymalnej strategii. Uczenie siê polega na oszacowaniu optymalnej funkcji warto¶ci akcji. W ka¿dy kroku czasowym obliczana jest warto¶æ nastêpuj±cego wyra¿enia, które nazywane jest b³êdem:

\begin{equation}
\Delta = r_t + \gamma\max_aQ_t(x_{t+1},a) - Q_t(x_t,a_t);
\end{equation}

$r_t$ to warto¶æ wzmocnienia w tym kroku czasowym, nastêpny cz³on okre¶la maksymaln± warto¶æ funkcji $Q$ dla stanu, w którym znajdzie siê uczeñ po wykonaniu wybranej wcze¶niej przez siebie akcji. Tê warto¶æ mno¿ymy przez wspó³czynnik dyskontowania $\gamma$ i od tak obliczonej liczby odejmujemy obecn± warto¶æ funkcji dla obecnego stanu i wybranej akcji.\\
Aktualizacja warto¶ci funkcji odbywa siê w nastêpuj±cy sposób:

\begin{equation}
Q_{t+1}(x_t,a_t) = Q_t(x_t,a_t) + \alpha\Delta
\end{equation}

Dwa warunki dotycz±ce warto¶ci $\alpha$ powinny byæ spe³nione:

\begin{equation}
\sum_{i=1}^\infty\frac{1}{\alpha_i(x,a)} = \infty
\end{equation}
\begin{equation}
\sum_{i=1}^\infty\frac{1}{\alpha_i^2(x,a)} < \infty
\end{equation}

Jednak najczê¶ciej w praktyce rzadko stosuje siê do tych warunków.

Warto zwróciæ uwagê na fakt, ¿e nie okre¶lono tu w jaki sposób uczeñ rzeczywi¶cie wybiera akcjê. Nie musi to byæ wcale strategia, w której zawsze wybierana jest najlepsza z akcji.

\paragraph{Algorytm Sarsa.}
Algorytm Sarsa niewiele ró¿ni siê od algorytmu Q-learning. Jedyn± ró¿nic± jest modyfikacja wyra¿enia na b³±d:

\begin{equation}
\Delta = r_t + \gamma Q_t(x_{t+1},a_{t+1}) - Q_t(x_t,a_t);
\end{equation}

Zamiast wykorzystywaæ maksymaln± warto¶æ funkcji w przysz³ym stanie, korzystamy z warto¶ci, która faktycznie zostanie wybrana. Poniewa¿ wybór ten dokonywany jest dopiero w nastêpnym kroku, ogólny schemat musi byæ lekko zmodyfikowany. Ró¿nica ta powoduje, ¿e algorytm Sarsa nie posiada w³asno¶ci algorytmu Q-learning dotycz±cego wyboru akcji. Maksymalizacja dokonywana jest z u¿yciem tej samej strategii, z której korzysta algorytm.

\subsubsection{Wybór akcji}
Tak jak w przypadku algorytmów ewolucyjnych konieczna jest eksploracja ¶rodowiska, w którym znajduje siê uczeñ. Z drugiej jednak strony chcemy jak najszybciej znale¼æ optymalne zachowanie, czyli dokonaæ eksploatacji wiedzy, któr± ju¿ posiadamy. Pojawia siê pytanie w jaki sposób wybieraæ akcjê, aby dobrze poznaæ ¶rodowisko, a jednocze¶nie nie zmarnowaæ zbyt du¿o czasu na bezcelowe przeszukiwanie opcji, które nie daj± korzystnego rozwi±zania.\\
W przypadku algorytmów takich jak Q-learning, w których mo¿na u¿yæ innej strategii wyboru akcji ni¿ ta, która jest u¿ywana w trakcie maksymalizacji funkcji $Q$, zapewnienie eksploracji mo¿na zapewniæ stosuj±c jednostajnie losowy wybór akcji.

% wg http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node16.html 
% http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node17.html
Najprostszym sposobem wyboru akcji jest wybranie najlepszej z mo¿liwych akcji, jednak metoda ta uniemo¿liwia eksploracjê. Aby to umo¿liwiæ, wprowadza siê modyfikacjê, która polega na tym, ¿e co jaki¶ czas z prawdopodobieñstwem $\epsilon$ stosuje siê wspomniany wcze¶niej jednostajnie losowy wybór akcji. Strategia ta nazywana jest strategi± $\epsilon$-zach³ann±. Metoda ta jest znana jako dobrze równowa¿±ca zadania eksploracji i eksploatacji.

Minusem strategii $\epsilon$-zach³annej jest to, ¿e w trakcie eksploracji ka¿da z akcji jest losowana z takim samym prawdopodobieñstwem. Aby wyeliminowaæ ten element, stosuje siê metody selekcji nazywane \textit{softmax}. Wszystkie akcje mog± zostaæ wylosowane, ale z prawdopodobieñstwem odpowiadaj±cym im dotychczasowej warto¶ci. Najczê¶ciej korzysta siê z rozk³adu Gibbs'a lub Boltzmann'a. Prawdopodobieñstwo wybrania akcji $a$ w próbie $t$ wynosi:

\begin{equation}
\frac{\epsilon^{Q_t(a)/\tau}}{\sum_{b=1}^n\epsilon^{Q_t(b)/\tau'}}
\end{equation}

Parameter $\tau$ nazywany jest temperatur±. Im jest ona wy¿sza, tym wybór dowolnej akcji jest bardziej losowy. W przypadku temperatur bliskich 0 wybór praktycznie jednoznacznie padnie na akcjê o najwy¿szej warto¶ci funkcji $Q$. 