\chapter{Optymalizacja}
\label{sec:optymalizacja}

W tym rozdziale opisane s± metody optymalizacji u¿yte w zaproponowanym rozwi±zaniu, czyli algorytm ewolucyjny oraz algorytm optymalizacji lokalnej - Hill climbing.

Zadaniem optymalizacji jest przeszukanie przestrzeni rozwi±zañ w celu znalezienia najlepszego. Zatem, dana jest funkcja, nazywan± funkcj± celu, która ka¿demu punktowi reprezentuj±cemu rozwi±zanie problemu przypisuje jak±¶ warto¶æ oceniaj±c± jego jako¶æ. W¶ród wszystkich rozwi±zañ poszukujemy takiego, dla którego warto¶æ tej funkcji bêdzie jak najmniejsza (b±d¼ jak najwiêksza) - najlepsza z naszego punktu widzenia. Trudno¶æ w znalezieniu takiego rozwi±zania zale¿y od charakteru funkcji celu, a czasem tak¿e od nieznajomo¶ci jej analitycznej postaci.

\section{Optymalizacja lokalna i globalna}
W przypadku funkcji z jednym optimum do znalezienia najlepszego rozwi±zania wystarczy przeszukiwanie lokalne. Polega ono na iteracyjnym sprawdzaniu rozwi±zañ w najbli¿szej przestrzeni i wprowadzaniu lokalnych zmian, aby w koñcu znale¼æ rozwi±zanie najlepsze w okolicy tzw. optimum lokalne. Je¶li wiemy, ¿e istnieje tylko jedno takie optimum, mo¿emy mieæ pewno¶æ, ¿e znalezione rozwi±zanie jest najlepszym w ca³ej przestrzeni rozwi±zañ. Przyk³adem optymalizacji lokalnej jest algorytm Hill Climbing.

Je¶li natomiast funkcja celu posiada wiele optimów lokalnych (tzw. funkcja wielomodalna) to optymalizacjê nazywamy optymalizacj± globaln±. Je¶li zadanie jest ci±g³e, a wiêc niemo¿liwe jest przeszukanie ca³ej przestrzeni rozwi±zañ, nigdy nie mo¿emy byæ pewni, ¿e zastosowany algorytm optymalizacji da nam rozwi±zanie najlepsze - byæ mo¿e bêdzie to tylko minimum lokalne, a nie globalne. Nie maj±c takiej pewno¶ci nie wiemy kiedy nale¿y zatrzymaæ algorytm. Z tego powodu stosuje siê parametr steruj±cy czasem trwania obliczeñ - kosztem mniejszej pewno¶ci co do poprawno¶ci rozwi±zania mo¿emy otrzymaæ krótszy czas optymalizacji i odwrotnie.

\section{Algorytm ewolucyjny}
\label{sec:ewolucyjny}
Algorytm ewolucyjny jest przyk³adem algorytmu optymalizacyjnego, przeszukuj±cego przestrzeñ rozwi±zañ w celu znalezienia najlepszego rozwi±zania problemu. Algorytm ten bazuje na obserwacjach ¶rodowiska i przystosowywania siê organizmów do jego warunków. Wiele terminów zapo¿yczonych jest zatem z genetyki.

``Algorytm ewolucyjny przetwarza populacjê osobników, z których ka¿dy jest propozycj± rozwi±zania postawionego problemu. Dzia³a on w ¶rodowisku, które mo¿na zdefiniowaæ na podstawie problemu rozwi±zywanego przez algorytm. W ¶rodowisku ka¿demu osobnikowi jest przyporz±dkowana warto¶æ liczbowa, okre¶laj±ca jako¶æ reprezentowanego przez niego rozwi±zania; warto¶æ ta jest nazywana przystosowaniem osobnika.'' \cite{arabas}

Funkcja przystosowania okre¶la warto¶æ przystosowania osobnika na podstawie jego fenotypu, który jest tworzony z genotypu. Genotyp okre¶la zestaw cech danego osobnika i sk³ada siê z chromosomów (najczê¶ciej z jednego). Natomiast ka¿dy z chromosomów sk³ada siê z elementarnych jednostek - genów.\\

\paragraph{Schemat dzia³ania algorytmu ewolucyjnego}
Na pocz±tku dzia³ania algorytmu ewolucyjnego generowana jest populacja bazowa oraz obliczane przystosowanie ka¿dego z jej osobników. Przewa¿nie osobniki te generowane s± ca³kowicie losowo, ale mo¿na tak¿e wprowadziæ konkretne osobniki np. o znanym dobrym przystosowaniu do ¶rodowiska.

G³ówna czê¶æ algorytmu opiera siê na powtarzaniu pêtli, w której wykonywane s± kolejne kroki przedstawione w \ref{alg:glowny}.

\begin{algorithm}
\caption{Schemat dzia³ania algorytmu ewolucyjnego}
\label{alg:glowny}
\begin{algorithmic}
\STATE	$reprodukcja$
\STATE	$operacje$ $genetyczne$
\STATE	$ocena$
\STATE	$sukcesja$
\end{algorithmic}
\end{algorithm}


Czêsto reprodukcjê i sukcesjê ³±czy siê pod nazw± selekcja.

Reprodukcja powoduje powielenie losowo wybranych osobników z populacji. Prawdopodobieñstwo wybrania osobnika do powielenia najczê¶ciej jest proporcjonalne do jego przystosowania. Mo¿e siê zdarzyæ, ¿e dany osobnik zostanie wybrany wiêcej ni¿ raz, a tak¿e, ¿e nie zostanie wybrany ani razu.\\
Nastêpnie na tych kopiach przeprowadzane s± operacje genetyczne powoduj±ce zmiany w genotypie osobników. Wyró¿niamy dwie podstawowe operacje:

\begin{itemize}
\item mutacja
\item krzy¿owanie
\end{itemize}

Zadaniem mutacji jest losowe zmodyfikowanie genów w genotypie.\\
Krzy¿owanie, zwane tak¿e rekombinacj± (ang. \textit{crossover}), dzia³a na co najmniej dwóch osobnikach i na podstawie ich genotypów tworzy jeden lub wiêcej osobników potomnych. Chromosomy rodzicielskie s± mieszane w celu otrzymania nowych genotypów dla osobników potomnych.

W wyniku operacji genetycznych powstaj± nowe osobniki, które wchodz± w sk³ad populacji potomnej. Ka¿dy z tych osobników jest oceniany za pomoc± funkcji przystosowania. Porównuj±c jako¶æ osobników z populacji bazowej oraz potomnej dokonuje siê sukcesji, czyli wyboru osobników z tych populacji (czasem wy³±cznie z populacji potomnej) i tworzy now± populacjê bazow±.

Decyzja o zakoñczeniu dzia³ania algorytmu przewa¿nie podejmowana jest w oparciu o badanie warto¶ci funkcji przystosowania ca³ej populacji. Je¶li warto¶æ przystosowania populacji nie jest zró¿nicowana mówimy o stagnacji algorytmu i mo¿e byæ to wskazaniem do zakoñczenia dzia³ania algorytmu. Czasem jednak oczekuje siê a¿ przystosowanie to bêdzie wystarczaj±co du¿e, ¿eby stwierdziæ, ¿e znalezione rozwi±zanie jest bardzo dobre. Przewa¿nie jednak nie znamy nawet przybli¿onej warto¶ci przystosowania rozwi±zania, wiêc nie mo¿emy stwierdziæ kiedy przystosowanie jest odpowiednie i czy nie mo¿e siê jeszcze znacznie poprawiæ.

\paragraph{Kodowanie osobników}
W przypadku algorytmów genetycznych do kodowania osobników stosuje siê kodowanie binarne chromosomów. Pojedynczy bit reprezentuje zatem gen nale¿±cy do chromosomu.\\
W takim przypadku mutacja wykonywana jest na ka¿dym genie osobno z pewnym prawdopodobieñstwem, je¶li do niej dochodzi, zmienia siê warto¶æ bitu na przeciwn±. W krzy¿owaniu wybiera siê dwa osobniki rodzicielskie, których chromosomy rozcinane s± na dwie czê¶ci i ³±czone ``na krzy¿''. Miejsce przeciêcia jest losowane z rozk³adem równomiernym.

W algorytmach ewolucyjnych porzuca siê kodowanie binarne - chromosom sk³ada siê z jednej lub wiêcej liczb stanowi±cych cechy osobnika.\\
Mutacja takiego osobnika najczê¶ciej odbywa siê poprzez losow± zmianê ka¿dej z warto¶ci genów chromosomu. Do krzy¿owania wybiera siê dwa osobniki, z których dla ka¿dej pary odpowiadaj±cych genów wyci±gana jest ¶rednia i tak otrzymane warto¶ci genów tworz± genotyp nowego osobnika.

\paragraph{Typy algorytmów ewolucyjnych}
Algorytmy ewolucyjne wywodz± siê z kilku osobnych nurtów zajmuj±cych siê t± tematyk±, wiêc istnieje wiele podobnych schematów dzia³ania. Najlepiej traktowaæ algorytmy ewolucyjne jako metaheurystykê - okre¶lony jest pewien szkic algorytmu, który mo¿na dostosowywaæ do konkretnego rozwi±zania. W tym podrozdziale opisane s± podstawowe i najbardziej popularne schematy dzia³ania algorytmów ewolucyjnych.\\

\begin{enumerate}

\item{Prosty algorytm genetyczny}

Prosty algorytm genetyczny zosta³ zaproponowany w roku 1975 przez John'a Holland'a w \cite{Holland1975}.

Poni¿ej umieszczony jest schemat tego algorytmu (Algorytm \ref{alg:prostyGenetyczny} na podstawie algorytmu umieszczonego w \cite{arabas}).

\begin{algorithm}
\caption{Prosty algorytm genetyczny}
\label{alg:prostyGenetyczny}
\begin{algorithmic}
\STATE	$t = 0$
\STATE	$P^0 = createInitPop() $
\WHILE {$stopCondition == false$}
\STATE	$T^t = createTempPop(P^t) $
\STATE	$T^t = crossPop(T^t) $
\STATE	$O^t = mutatePop(T^t) $
\STATE	$P^{t+1} = O^t$
\STATE	$t=t+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


Maj±c populacjê bazow± $P^t$ dokonujemy reprodukcji tej populacji, tworz±c populacjê tymczasow± $T^t$ sk³adaj±c± siê z takiej samej liczby osobników. Wybierani s± oni z prawdopodobieñstwem proporcjonalnym do ich przystosowania z populacji bazowej. Na populacji tymczasowej dokonujemy operacji genetycznych (mutacji i krzy¿owania). Do krzy¿owania wybierane s± roz³±czne pary osobników i z pewnym prawdopodobieñstwem $p_c$ zachodzi ich skrzy¿owanie. Je¶li dosz³o do powstania osobników potomnych zastêpuj± one osobniki rodzicielskie. Nastêpnie na tak otrzymanej populacji tymczasowej dochodzi do mutacji osobników i otrzymania populacji potomnej $O^t$. Ta populacja staje siê w nastêpnej iteracji algorytmu now± populacj± bazow±.\\
Zatrzymanie algorytmu mo¿e byæ dokonane je¶li np.:

\begin{itemize}
\item wykonano okre¶lon± z góry liczbê iteracji
\item znaleziono osobnika o wystarczaj±co wysokiej warto¶ci przystosowania
\end{itemize}

W tej wersji algorytmu czêsto pêtlê algorytmu nazywa siê generacj±, a ka¿d± populacjê $P^t$ w chwili t pokoleniem.\\
\\

\item{Strategia (1+1)}

Strategia (1+1) jest strategi± ewolucyjn± zaproponowan± przez Schwefel'a w 1965 r. \cite{Schwefelthesis}, a pó¼niej analizowana równie¿ w pracach Rechenberg'a \cite{rechenberg1971}, \cite{rechenberg1973}. W algorytmie tym mamy do czynienia z populacj± sk³adaj±c± siê tylko z jednego osobnika posiadaj±cego jeden chromosom. W ka¿dej pêtli algorytmu dokonuje siê mutacji tego chromosomu, co powoduje powstanie nowego osobnika. Osobnik ten jest poddawany ocenie, a nastêpnie dokonuje siê wyboru lepszego z dwóch istniej±cych osobników i tego pozostawia w populacji.\\
W mutacji dodaje siê do ka¿dego genu chromosomu losow± modyfikacjê rozk³adem normalnym:
\begin{equation}
Y^t_i = X^t_i + \sigma\xi_{N(0,1),i}
\end{equation}

Warto¶æ $\sigma$ bêdzie powodowa³a wiêksze lub mniejsze zmiany w chromosomie. Je¶li chcemy przeszukaæ przestrzeñ rozwi±zañ, powinni¶my zwiêkszaæ jej warto¶æ, co jest po¿±dane zw³aszcza w pocz±tkowej fazie dzia³ania algorytmu. Natomiast, aby znale¼æ jak najlepsze rozwi±zanie, wiedz±c ¿e obecne rozwi±zanie jest ju¿ bardzo bliskie najlepszemu, mo¿emy zmniejszaæ warto¶æ $\sigma$ przeszukuj±c tylko najbli¿sz± przestrzeñ.\\
Do wyznaczania $\sigma$ powsta³ nastêpuj±cy algorytm zwany regu³± 1/5 sukcesów (\cite{rechenberg1973}):
\begin{enumerate}
\item Je¶li przez kolejnych k pêtli algorytmu mutacja powoduje powstanie lepszego osobnika w wiêcej ni¿ 1/5 wszystkich mutacji, to zwiêkszamy $\sigma$: $\sigma' = c_i \sigma$. Warto¶æ $c_i$ wyznaczona empirycznie wynosi $ \frac{1}{0.82} $ (\cite{schwefel1981})
\item Gdy dok³adnie 1/5 koñczy siê sukcesem, warto¶æ $\sigma$ pozostaje bez zmian.
\item Je¶li nie zachodzi ¿adne z powy¿szych warto¶æ $\sigma$ jest zmniejszana: $\sigma' = c_d \sigma$. Gdzie $ c_d $ powinna wynosiæ $ 0.82 $ (\cite{schwefel1981})
\end{enumerate}

Opisane poni¿ej strategie s± rozwiniêciem strategii (1+1) i zosta³y zaproponowane przez Schwefel'a w \cite{Schwefel1975}, \cite{Schwefel1977}, \cite{schwefel1981}.

\item{Strategia ($\mu$ + $\lambda$)}

Strategia ($\mu$ + $\lambda$) jest rozwiniêciem strategii (1+1). $\mu$ oznacza ilo¶æ osobników w populacji pocz±tkowej, a $\lambda$ ile osobników jest reprodukowanych i poddawanych operacjom genetycznym. Dodatkowo, zamiast regu³y 1/5 sukcesów wprowadzono mechanizm samoczynnej adaptacji zasiêgu mutacji, a tak¿e wprowadzono operator krzy¿owania.

Oznaczenie $\mu$ + $\lambda$ oznacza, ¿e po wygenerowaniu populacji potomnej wybierane jest $\mu$ najlepszych osobników do nowej populacji bazowej - zarówno spo¶ród populacji potomnej, jak i starej populacji bazowej zawieraj±cych ³±cznie $\mu$ + $\lambda$ osobników. Algorytm \ref{alg:miPlusLambda} przedstawia schemat tej strategii (bazuj±c na algorytmie umieszczonym w \cite{arabas}).

\begin{algorithm}
\caption{Strategia ewolucyjna ($\mu$ + $\lambda$)}
\label{alg:miPlusLambda}
\begin{algorithmic}
\STATE	$t = 0$
\STATE	$P^0 = createInitPop(\mu) $
\WHILE {$stopCondition == false$}
\STATE	$T^t = createTempPop(P^t,\lambda) $
\STATE	$T^t = crossPop(T^t) $
\STATE	$O^t = mutatePop(T^t) $
\STATE	$P^{t+1} = select(O^t \cup P^t,\mu)$
\STATE	$t=t+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


W strategii tej wa¿ne jest te¿ kodowanie, do którego dodatkowo do³o¿ono chromosom przechowuj±cy wektor $\sigma$ zawieraj±cy warto¶ci odchyleñ standardowych, które wykorzystuje siê w trakcie mutacji.\\
Po wylosowaniu warto¶ci zmiennej losowej o rozk³adzie normalnym ($\xi_{N(0,1)}$) dla ka¿dego elementu wektora $\sigma$ losujemy jeszcze jedn± zmienn± losow± o rozk³adzie normalnym ($\xi_{N(0,1),i}$) i obliczamy nowe warto¶ci odchyleñ z wektora $\sigma$:

\begin{equation}
\sigma'_i = \sigma_i e^{(\tau'\xi_{N(0,1)} + \tau\xi_{N(0,1),i})}
\end{equation}


Gdzie $\tau$ oraz $\tau'$ s± parametrami algorytmu, a ich warto¶ci powinny wynosiæ wed³ug \cite{schwefel1995}:
\begin{equation}
\tau = \frac{K}{\sqrt{2n}}
\end{equation}

\begin{equation}
\tau' = \frac{K}{\sqrt{2\sqrt{n}}}
\end{equation}

gdzie:
\begin{itemize}
\item K - sta³a, najczê¶ciej stosuje siê warto¶æ 1 (\cite{schwefel1995})
\item n - wymiarowo¶æ zadania
\end{itemize}

Beyer w 2007 r. napisa³, ¿e empirycznie sprawdzono, ¿e nale¿y wybieraæ warto¶æ $\tau$ proporcjonalnie do $\frac{1}{\sqrt{n}}$. \cite{Beyer07}

Maj±c dane nowe warto¶ci odchyleñ standardowych mo¿emy obliczyæ nowe warto¶ci genów korzystaj±c ze wzoru:

\begin{equation}
X'_i = X_i + \sigma'_i\xi_{N(0,1),i}
\end{equation}
gdzie $\xi_{N(0,1),i}$ jest now± losow± warto¶ci±.

Algorytm ewolucyjny wybiera osobniki lepiej przystosowane, uwzglêdniaj±c przy tym równie¿ warto¶ci odchyleñ standardowych. Powoduje to naturaln± selekcjê, doprowadzaj±c± do samoczynnej adaptacji odchyleñ standardowych stosowanych w trakcie mutacji.

Krzy¿owanie wystêpuje w tym algorytmie pod nazw± ``rekombinacja''. Najczê¶ciej sprowadza siê do u¶rednienia lub wymianie warto¶ci sk³adowe wektorów, tak¿e warto¶ci $\sigma$.

\item{Strategia ($\mu$, $\lambda$)}

Strategia ($\mu$ + $\lambda$) posiada pewne wady, które wyeliminowano za pomoc± nowej strategii ($\mu$, $\lambda$). Poprzedni algorytm sprawia problemy je¶li w populacji pojawia siê osobnik o wysokiej warto¶ci przystosowania, ale posiadaj±cy zbyt du¿e (albo zbyt ma³e) warto¶ci odchyleñ standardowych. Usuniêcie takiego osobnika z populacji czêsto nie jest procesem krótkotrwa³ym, gdy¿ wp³ywa on na powstaj±ce potomstwo, przekazuj±c mu podobne do jego, nieodpowiednie warto¶ci odchyleñ.\\
W nowej strategii wprowadzono zmianê, która powoduje, ¿e osobniki rodzicielskie nie s± nigdy brane do kolejnej populacji bazowej. Podczas selekcji korzysta siê zatem tylko z powsta³ej populacji potomnej, z niej wybieraj±c osobniki do populacji bazowej w kolejnej iteracji. Algorytm \ref{alg:miLambda} prezentuje kolejne kroki schematu tego algorytmu (bazuj±c na algorytmie umieszczonym w \cite{arabas}).

\begin{algorithm}
\caption{Strategia ewolucyjna ($\mu$, $\lambda$)}
\label{alg:miLambda}
\begin{algorithmic}
\STATE	$t = 0$
\STATE	$P^0 = createInitPop(\mu) $
\WHILE {$stopCondition == false$}
\STATE	$T^t = createTempPop(P^t,\lambda) $
\STATE	$T^t = crossPop(T^t) $
\STATE	$O^t = mutatePop(T^t) $
\STATE	$P^{t+1} = select(O^t,\mu)$
\STATE	$t=t+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\end{enumerate}

\section{Algorytm memetyczny}
Richard Dawkins zaproponowa³ w \cite{dawkins89} pojêcie memu - ``the basic unit of cultural transmission, or imitation''. Algorytmy ewolucyjne opieraj± siê na ewolucji biologicznej, Dawkins zaproponowa³, ¿e istnieje równie¿ rozwój kulturowy. Na tej podstawie wprowadzono algorytmy memetyczne, które stanowi± rozwiniêcie algorytmów ewolucyjnych poprzez dodanie do nich lokalnej optymalizacji. Mo¿e byæ ona wprowadzana na ró¿nych etapach algorytmu (\cite{ChenOLT11}):

\begin{itemize}
\item na pocz±tku, w celu wygenerowanie potencjalnie lepszej populacji pocz±tkowej
\item w fazie oceny przystosowania
\item po mutacji i krzy¿owaniu
\item po zakoñczeniu fazy ewolucyjnej do ostatecznego poprawienia rozwi±zania
\end{itemize}

£±cz±c te dwa algorytmy ze sob±, mo¿emy wykorzystaæ i po³±czyæ ich zalety. Uwzglêdnienie algorytmu lokalnej optymalizacji pomaga w skutecznym znajdowaniu optimum lokalnego, jednak aby móc przeszukiwaæ wszystkie optima w celu znalezienia najlepszego - globalnego, potrzebujemy eksploracji jak± daje nam algorytm ewolucyjny.

\section{Hill Climbing}
\label{sec:hill}
Podrozdzia³ bazuje na opisie algorytmu Hill Climbing w \cite{russell}.

Algorytm Hill Climbing jest jedn± z metod przeszukiwania lokalnego. W podstawowej wersji algorytmu zwanej \textit{Greedy local search}, w ka¿dej iteracji zmieniaj±c warto¶æ rozwi±zania w jednym z wymiarów sprawdzana jest warto¶æ funkcji celu dla nowego rozwi±zania i je¶li warto¶æ ta jest lepsza od dotychczas najlepszej znalezionej, zapamiêtujemy zmienione rozwi±zanie. Dopóki zmiany powoduj± poprawê rozwi±zania, algorytm nie jest zatrzymywany. Na koñcu wiemy, ¿e znalezione rozwi±zanie jest rozwi±zaniem lokalnie optymalnym.

Istnieje wiele rodzajów algorytmu rozwijaj±cych wersjê podstawow±:

\begin{itemize}
\item \textit{Stochastic hill climbing} - wybiera kolejny punkt losowo, prawdopodobieñstwo wyboru mo¿e zale¿eæ od tego jak du¿a poprawa wi±¿e siê z wybraniem danego punktu,
\item \textit{First-choice hill climbing} - wybiera pierwszego z generowanych kolejno s±siadów, którego warto¶æ jest lepsza ni¿ obecny stan. Rozwi±zanie to warto stosowaæ je¶li istnieje wielu s±siadów,
\item \textit{Random-restart hill climbing} - uruchamiaj±c wielokrotnie algorytm rozpoczynaj±c w losowych punktach mo¿emy zwiêkszyæ prawdopodobieñstwo znalezienia rozwi±zania globalnego.
\end{itemize}

Przeszukiwanie przestrzeni dyskretnej sprowadza siê do sprawdzania rozwi±zañ najbli¿szych obecnemu i wybieranie tego rozwi±zania, którego warto¶æ obliczona za pomoc± funkcji celu jest najlepsza. Je¶li w¶ród s±siadów nie ma ju¿ lepszego rozwi±zania, mo¿emy zakoñczyæ przeszukiwanie. Pseudokod algorytmu przedstawiony jest poni¿ej (Algorytm \ref{alg:hillClimbDiscrete}).\\

\begin{algorithm}
\caption{Hill Climbing w przestrzeni dyskretnej}
\label{alg:hillClimbDiscrete}
\begin{algorithmic}
\STATE $current = startPoint$
\STATE $foundBetter = true $
\WHILE {$ foundBetter == true $}
  \STATE $ foundBetter = false $
  \STATE $ neighbours = getNeighbours(current) $
  \FORALL {$neighbour$ $in$ $neighbours$}
    \IF {$ neighbour.isBetterThan(current) $}
      \STATE $ current = neighbour $
      \STATE $ foundBetter = true $
    \ENDIF
  \ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}


W przestrzeni ci±g³ej konieczne jest dobranie kroku, który wyznacza punkty przeszukiwane w okolicy w trakcie ka¿dej iteracji. Dodatkowo wykorzystywane jest tzw. przyspieszenie (ang. \textit{acceleration}), które wyznacza piêciu mo¿liwych kandydatów na lepsze rozwi±zania. Najczê¶ciej przyspieszenie to wynosi 1.2, a warto¶æ kroku jest osobna dla ka¿dej zmiennej rozwi±zania i czêsto wynosi na pocz±tku 1. Zatem za ka¿dym razem obliczane s± nastêpuj±ce wspó³czynniki: -acceleration, -1/acceleration, 0, 1/acceleration, acceleration. Nastêpnie wspó³czynniki mno¿one s± przez krok (step) i dodawane do obecnie analizowanej zmiennej i wybierane jest najlepsze z piêciu rozwi±zañ. Warto¶æ kroku jest indywidualna dla ka¿dej zmiennej. Po wybraniu najlepszego rozwi±zania uaktualniana jest warto¶æ tego kroku - krok mno¿ony jest przez odpowiedni wspó³czynnik, ten który by³ dobrany wcze¶niej do znalezienia tego najlepszego rozwi±zania. Algorytm zatrzymywany jest je¶li zmiana ¿adnej ze zmiennych nie przynosi ju¿ poprawy rozwi±zania, czasem równie¿ je¶li ta zmiana jest ju¿ bardzo ma³a - wprowadzany jest parametr $\epsilon$ wyznaczaj±cy tê ró¿nicê. Pseudokod algorytmu przedstawiony jest na stronie \pageref{alg:hillClimbCont} (Algorytm \ref{alg:hillClimbCont}).


\begin{algorithm}
\caption{Hill Climbing w przestrzeni ci±g³ej}
\label{alg:hillClimbCont}
\begin{algorithmic}
\STATE $ currentResult = startPoint$
\STATE $ steps = initialSteps $ \COMMENT {for each dimension of the solution}
\STATE $ candidates = [-acc, -\frac{1}{acc}, 0, \frac{1}{acc}, acc] $
\STATE $ currentValue = currentResult.getValue() $
\STATE $ beforeValue = MAX\_VALUE $
\STATE $ \epsilon = EPSILON $
\WHILE {$ beforeValue - currentValue > \epsilon $}
  \STATE $ beforeValue = currentValue $
  \FOR {$i$ $in$ $dimensions$}
    \STATE $ bestIndex = -1 $
	\STATE $ bestScore = MAX\_VALUE $\
	\FOR {$j$ $in$ $candidatesNrs$} 
	  \STATE $ currentResult[i] = currentResult[i] + stepSize[i] * candidates[j] $
	  \STATE $ tempValue = currentResult.getValue() $
	  \STATE $ currentResult[i] = currentResult[i] - stepSize[i] * candidates[j] $
	  \IF {$tempValue.isBetterThan(bestScore)$}
	    \STATE $ bestScore = tempValue $
	    \STATE $ bestIndex = j $
	  \ENDIF
	\ENDFOR
	\IF {$ candidates[bestIndex]!=0 $}
   	  \STATE $ currentResult[i] = currentResult[i] + stepSize[i] * candidates[bestIndex] $
	  \STATE $ stepSize[i] = stepSize[i] * candidates[bestIndex] $    	  \COMMENT {accelerate}
	\ENDIF
  \ENDFOR
  \STATE $ currentValue = bestScore $
\ENDWHILE
\end{algorithmic}
\end{algorithm}
